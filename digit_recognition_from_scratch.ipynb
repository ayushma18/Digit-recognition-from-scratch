{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35066ac4",
   "metadata": {},
   "source": [
    "---\n",
    "## âš¡ QUICK START: Upload and Predict Your Image\n",
    "\n",
    "**Want to test immediately?** Follow these simple steps:\n",
    "\n",
    "1. **Save your handwritten digit image** in this notebook's directory\n",
    "2. **Update the filename** in the cell below\n",
    "3. **Run the cell** to get your prediction!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18edd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ðŸŽ¯ UPLOAD YOUR IMAGE HERE\n",
    "# ================================================================\n",
    "#\n",
    "# INSTRUCTIONS:\n",
    "# 1. Place your handwritten digit image (0-9) in this directory\n",
    "# 2. Update the filename below (e.g., \"my_digit.png\", \"test.jpg\")\n",
    "# 3. Set INVERT_COLORS:\n",
    "#    - True: if your digit is BLACK on WHITE background (most common)\n",
    "#    - False: if your digit is WHITE on BLACK background (like MNIST)\n",
    "# 4. Run this cell!\n",
    "#\n",
    "# NOTE: This cell will work AFTER you complete the training below.\n",
    "# ================================================================\n",
    "\n",
    "# YOUR IMAGE FILENAME HERE:\n",
    "IMAGE_FILENAME = \"my_digit.png\"  # Change this to your image filename\n",
    "\n",
    "# Color setting:\n",
    "INVERT_COLORS = True  # True for black digit on white background\n",
    "\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "# Check if image exists\n",
    "if os.path.exists(IMAGE_FILENAME):\n",
    "    print(\"âœ… Image found!\")\n",
    "    print(f\"ðŸ“ File: {IMAGE_FILENAME}\")\n",
    "    print(\"\\nâš ï¸ Make sure you have trained the model first by running all cells below.\")\n",
    "    print(\"\\nðŸ“Œ After training, re-run this cell to see your prediction!\")\n",
    "    \n",
    "    # Try to make prediction (will work after training)\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        import numpy as np\n",
    "        \n",
    "        # Display the uploaded image\n",
    "        img = Image.open(IMAGE_FILENAME)\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(\"Your Uploaded Image\", fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Try prediction if model is trained\n",
    "        if 'trained_parameters' in dir():\n",
    "            predicted_digit, confidence, probabilities, _, _ = predict_digit(\n",
    "                IMAGE_FILENAME, \n",
    "                trained_parameters, \n",
    "                invert=INVERT_COLORS\n",
    "            )\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(f\"The uploaded handwritten digit is recognized as: {predicted_digit}\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"Confidence: {confidence*100:.2f}%\")\n",
    "            print(\"\\nTop 3 predictions:\")\n",
    "            top_3 = np.argsort(probabilities)[-3:][::-1]\n",
    "            for i, idx in enumerate(top_3, 1):\n",
    "                print(f\"  {i}. Digit {idx}: {probabilities[idx]*100:.2f}%\")\n",
    "            print(\"=\"*70)\n",
    "            \n",
    "            # Display with visualization\n",
    "            display_prediction(IMAGE_FILENAME, trained_parameters, invert=INVERT_COLORS)\n",
    "        else:\n",
    "            print(\"\\nâš ï¸ Model not trained yet. Please run all cells below to train the model first.\")\n",
    "            \n",
    "    except NameError:\n",
    "        print(\"\\nâš ï¸ Prediction functions not loaded yet.\")\n",
    "        print(\"Please run all cells in this notebook first to:\")\n",
    "        print(\"  1. Define the prediction functions\")\n",
    "        print(\"  2. Train the neural network\")\n",
    "        print(\"  3. Then come back and re-run this cell\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error: {str(e)}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"âŒ Image file '{IMAGE_FILENAME}' not found!\")\n",
    "    print(\"\\nðŸ“‹ To upload your image:\")\n",
    "    print(\"  1. Save your handwritten digit image (PNG, JPG, JPEG)\")\n",
    "    print(\"  2. Place it in this notebook's directory:\")\n",
    "    print(f\"     {os.getcwd()}\")\n",
    "    print(f\"  3. Update IMAGE_FILENAME variable above\")\n",
    "    print(\"  4. Run this cell again\")\n",
    "    print(\"\\nðŸ’¡ You can also drag-and-drop the image file into the file browser.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8880edc",
   "metadata": {},
   "source": [
    "# Handwritten Digit Recognition from Scratch\n",
    "\n",
    "**Built Without Deep Learning Libraries**\n",
    "\n",
    "This notebook implements a complete neural network from scratch to recognize handwritten digits (0-9) from images. \n",
    "\n",
    "## Key Features:\n",
    "- No TensorFlow, PyTorch, or Keras\n",
    "- Pure algorithmic implementation using NumPy\n",
    "- Detailed mathematical explanations\n",
    "- Custom image input support\n",
    "- Output format: \"The uploaded handwritten digit is recognized as: X\"\n",
    "\n",
    "## Architecture:\n",
    "- Input Layer: 784 neurons (28x28 pixels)\n",
    "- Hidden Layer: 128 neurons (ReLU activation)\n",
    "- Output Layer: 10 neurons (Softmax activation)\n",
    "- Training: Backpropagation with mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a781ed2",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll use only basic libraries:\n",
    "- **NumPy**: For numerical operations and matrix computations\n",
    "- **Matplotlib**: For visualization\n",
    "- **PIL (Pillow)**: For image loading and preprocessing\n",
    "- **urllib**: For downloading MNIST dataset\n",
    "- **gzip**: For decompressing dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85326729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783fb78",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess the MNIST Dataset\n",
    "\n",
    "The MNIST dataset contains 60,000 training images and 10,000 test images of handwritten digits.\n",
    "\n",
    "### Process:\n",
    "1. Download the dataset from the official source\n",
    "2. Extract images and labels\n",
    "3. Normalize pixel values from [0, 255] to [0, 1]\n",
    "4. Flatten 28x28 images into 784-dimensional vectors\n",
    "5. One-hot encode labels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9572f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mnist():\n",
    "    \"\"\"Download MNIST dataset from the internet with multiple mirror fallbacks\"\"\"\n",
    "    \n",
    "    # Multiple mirror URLs for reliability\n",
    "    mirror_urls = [\n",
    "        \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\",\n",
    "        \"http://yann.lecun.com/exdb/mnist/\",\n",
    "        \"https://ossci-datasets.s3.amazonaws.com/mnist/\",\n",
    "    ]\n",
    "    \n",
    "    files = {\n",
    "        \"train_images\": \"train-images-idx3-ubyte.gz\",\n",
    "        \"train_labels\": \"train-labels-idx1-ubyte.gz\",\n",
    "        \"test_images\": \"t10k-images-idx3-ubyte.gz\",\n",
    "        \"test_labels\": \"t10k-labels-idx1-ubyte.gz\"\n",
    "    }\n",
    "    \n",
    "    data_dir = \"mnist_data\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    for key, filename in files.items():\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            \n",
    "            # Try each mirror until one succeeds\n",
    "            downloaded = False\n",
    "            for mirror_url in mirror_urls:\n",
    "                try:\n",
    "                    full_url = mirror_url + filename\n",
    "                    print(f\"  Trying: {mirror_url}...\")\n",
    "                    urllib.request.urlretrieve(full_url, filepath)\n",
    "                    print(f\"  âœ“ Downloaded {filename}\")\n",
    "                    downloaded = True\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"  âœ— Failed: {type(e).__name__}\")\n",
    "                    continue\n",
    "            \n",
    "            if not downloaded:\n",
    "                print(f\"\\n{'='*70}\")\n",
    "                print(f\"âŒ ERROR: Could not download {filename} from any mirror\")\n",
    "                print(f\"{'='*70}\")\n",
    "                print(f\"\\nðŸ”§ MANUAL DOWNLOAD INSTRUCTIONS:\")\n",
    "                print(f\"\\n1. Visit one of these URLs in your browser:\")\n",
    "                for mirror in mirror_urls:\n",
    "                    print(f\"   {mirror}{filename}\")\n",
    "                print(f\"\\n2. Save the file to: {os.path.abspath(filepath)}\")\n",
    "                print(f\"\\n3. Re-run this cell after downloading all 4 files\")\n",
    "                print(f\"{'='*70}\\n\")\n",
    "                raise Exception(f\"Failed to download {filename}. Please download manually.\")\n",
    "        else:\n",
    "            print(f\"âœ“ {filename} already exists\")\n",
    "    \n",
    "    return data_dir\n",
    "\n",
    "def load_mnist_images(filepath):\n",
    "    \"\"\"Load MNIST images from compressed file\"\"\"\n",
    "    with gzip.open(filepath, 'rb') as f:\n",
    "        # Read header information\n",
    "        magic = int.from_bytes(f.read(4), 'big')\n",
    "        num_images = int.from_bytes(f.read(4), 'big')\n",
    "        rows = int.from_bytes(f.read(4), 'big')\n",
    "        cols = int.from_bytes(f.read(4), 'big')\n",
    "        \n",
    "        # Read image data\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        data = data.reshape(num_images, rows, cols)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_mnist_labels(filepath):\n",
    "    \"\"\"Load MNIST labels from compressed file\"\"\"\n",
    "    with gzip.open(filepath, 'rb') as f:\n",
    "        # Read header information\n",
    "        magic = int.from_bytes(f.read(4), 'big')\n",
    "        num_labels = int.from_bytes(f.read(4), 'big')\n",
    "        \n",
    "        # Read label data\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def preprocess_data(images, labels):\n",
    "    \"\"\"\n",
    "    Preprocess images and labels for neural network\n",
    "    \n",
    "    Steps:\n",
    "    1. Flatten 28x28 images to 784-dimensional vectors\n",
    "    2. Normalize pixel values from [0, 255] to [0, 1]\n",
    "    3. One-hot encode labels\n",
    "    \"\"\"\n",
    "    # Flatten images: (num_samples, 28, 28) -> (num_samples, 784)\n",
    "    X = images.reshape(images.shape[0], -1)\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X = X.astype(np.float32) / 255.0\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    # Example: label 3 -> [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "    num_classes = 10\n",
    "    Y = np.zeros((labels.shape[0], num_classes))\n",
    "    Y[np.arange(labels.shape[0]), labels] = 1\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# Download and load the dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING MNIST DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data_dir = download_mnist()\n",
    "\n",
    "# Load training data\n",
    "train_images = load_mnist_images(os.path.join(data_dir, \"train-images-idx3-ubyte.gz\"))\n",
    "train_labels = load_mnist_labels(os.path.join(data_dir, \"train-labels-idx1-ubyte.gz\"))\n",
    "\n",
    "# Load test data\n",
    "test_images = load_mnist_images(os.path.join(data_dir, \"t10k-images-idx3-ubyte.gz\"))\n",
    "test_labels = load_mnist_labels(os.path.join(data_dir, \"t10k-labels-idx1-ubyte.gz\"))\n",
    "\n",
    "print(f\"\\nRaw Data Shapes:\")\n",
    "print(f\"Training images: {train_images.shape}\")\n",
    "print(f\"Training labels: {train_labels.shape}\")\n",
    "print(f\"Test images: {test_images.shape}\")\n",
    "print(f\"Test labels: {test_labels.shape}\")\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, Y_train = preprocess_data(train_images, train_labels)\n",
    "X_test, Y_test = preprocess_data(test_images, test_labels)\n",
    "\n",
    "print(f\"\\nPreprocessed Data Shapes:\")\n",
    "print(f\"X_train: {X_train.shape} (flattened and normalized)\")\n",
    "print(f\"Y_train: {Y_train.shape} (one-hot encoded)\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"Y_test: {Y_test.shape}\")\n",
    "\n",
    "print(f\"\\nData Statistics:\")\n",
    "print(f\"Pixel value range: [{X_train.min():.2f}, {X_train.max():.2f}]\")\n",
    "print(f\"Number of classes: {Y_train.shape[1]}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22c688",
   "metadata": {},
   "source": [
    "### Alternative: Direct Download Links\n",
    "\n",
    "If automatic download fails, you can manually download the MNIST files:\n",
    "\n",
    "**Option 1 - GitHub Mirror:**\n",
    "```bash\n",
    "wget https://github.com/golbin/TensorFlow-MNIST/raw/master/mnist/data/train-images-idx3-ubyte.gz\n",
    "wget https://github.com/golbin/TensorFlow-MNIST/raw/master/mnist/data/train-labels-idx1-ubyte.gz\n",
    "wget https://github.com/golbin/TensorFlow-MNIST/raw/master/mnist/data/t10k-images-idx3-ubyte.gz\n",
    "wget https://github.com/golbin/TensorFlow-MNIST/raw/master/mnist/data/t10k-labels-idx1-ubyte.gz\n",
    "```\n",
    "\n",
    "**Option 2 - Use Kaggle:**\n",
    "Download from: https://www.kaggle.com/datasets/hojjatk/mnist-dataset\n",
    "\n",
    "**Option 3 - Direct URLs:**\n",
    "- https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
    "- https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
    "- https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
    "- https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
    "\n",
    "Save all files to the `mnist_data/` folder in this notebook's directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfa7abd",
   "metadata": {},
   "source": [
    "### ðŸš¨ If Download Fails - Quick Fix!\n",
    "\n",
    "If you see \"HTTP Error 404\" or download fails, run this terminal command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33182e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUICK FIX: Run this if automatic download fails\n",
    "# This uses a reliable Google Cloud Storage mirror\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"ðŸ”„ Attempting alternative download method...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run([sys.executable, \"download_mnist.py\"], \n",
    "                          capture_output=True, \n",
    "                          text=True, \n",
    "                          timeout=180)\n",
    "    print(result.stdout)\n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nâœ… Download successful! You can now continue with the next cells.\")\n",
    "    else:\n",
    "        print(result.stderr)\n",
    "        print(\"\\nâš ï¸ If this fails, see manual download instructions above.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ download_mnist.py not found.\")\n",
    "    print(\"\\nðŸ’¡ Alternative: Run this in your terminal:\")\n",
    "    print(\"   cd /home/ayushma/test/AI/Digit_Recognition\")\n",
    "    print(\"   python3 download_mnist.py\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"\\nðŸ’¡ Try running in terminal: python3 download_mnist.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd9638c",
   "metadata": {},
   "source": [
    "## 3. Visualize Sample Digits\n",
    "\n",
    "Let's visualize some sample handwritten digits from the training dataset to understand what our neural network will be learning from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 15 random samples from the training set\n",
    "num_samples = 15\n",
    "fig, axes = plt.subplots(3, 5, figsize=(12, 7))\n",
    "fig.suptitle('Sample Handwritten Digits from MNIST Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Random index\n",
    "    idx = np.random.randint(0, len(train_images))\n",
    "    \n",
    "    # Display the image\n",
    "    ax.imshow(train_images[idx], cmap='gray')\n",
    "    ax.set_title(f'Label: {train_labels[idx]}', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each image is 28x28 pixels representing a handwritten digit (0-9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b3b13",
   "metadata": {},
   "source": [
    "## 4. Initialize Neural Network Parameters\n",
    "\n",
    "### Network Architecture:\n",
    "- **Input Layer**: 784 neurons (28Ã—28 pixels)\n",
    "- **Hidden Layer 1**: 128 neurons with ReLU activation\n",
    "- **Output Layer**: 10 neurons (digits 0-9) with Softmax activation\n",
    "\n",
    "### Weight Initialization:\n",
    "We use **He initialization** for ReLU layers:\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in}}}\\right)$$\n",
    "\n",
    "Where $n_{in}$ is the number of input neurons to the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a912290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for a 2-layer neural network\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_size : int\n",
    "        Number of input features (784 for flattened 28x28 images)\n",
    "    hidden_size : int\n",
    "        Number of neurons in the hidden layer\n",
    "    output_size : int\n",
    "        Number of output classes (10 for digits 0-9)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    parameters : dict\n",
    "        Dictionary containing initialized weights and biases\n",
    "    \"\"\"\n",
    "    \n",
    "    # He initialization for ReLU activation (hidden layer)\n",
    "    # Multiply by sqrt(2/n) where n is the number of inputs\n",
    "    W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    \n",
    "    # Xavier initialization for output layer\n",
    "    # Multiply by sqrt(1/n) where n is the number of inputs\n",
    "    W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1.0 / hidden_size)\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    parameters = {\n",
    "        'W1': W1,  # Shape: (784, 128)\n",
    "        'b1': b1,  # Shape: (1, 128)\n",
    "        'W2': W2,  # Shape: (128, 10)\n",
    "        'b2': b2   # Shape: (1, 10)\n",
    "    }\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Define network architecture\n",
    "INPUT_SIZE = 784   # 28x28 pixels\n",
    "HIDDEN_SIZE = 128  # Hidden layer neurons\n",
    "OUTPUT_SIZE = 10   # 10 classes (digits 0-9)\n",
    "\n",
    "# Initialize parameters\n",
    "parameters = initialize_parameters(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NEURAL NETWORK PARAMETERS INITIALIZED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNetwork Architecture:\")\n",
    "print(f\"  Input Layer:  {INPUT_SIZE} neurons\")\n",
    "print(f\"  Hidden Layer: {HIDDEN_SIZE} neurons (ReLU)\")\n",
    "print(f\"  Output Layer: {OUTPUT_SIZE} neurons (Softmax)\")\n",
    "\n",
    "print(f\"\\nParameter Shapes:\")\n",
    "print(f\"  W1 (Input â†’ Hidden):  {parameters['W1'].shape}\")\n",
    "print(f\"  b1 (Hidden bias):     {parameters['b1'].shape}\")\n",
    "print(f\"  W2 (Hidden â†’ Output): {parameters['W2'].shape}\")\n",
    "print(f\"  b2 (Output bias):     {parameters['b2'].shape}\")\n",
    "\n",
    "# Calculate total number of parameters\n",
    "total_params = (parameters['W1'].size + parameters['b1'].size + \n",
    "                parameters['W2'].size + parameters['b2'].size)\n",
    "print(f\"\\nTotal trainable parameters: {total_params:,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf357a9",
   "metadata": {},
   "source": [
    "## 5. Implement Activation Functions\n",
    "\n",
    "### ReLU (Rectified Linear Unit)\n",
    "$$f(x) = \\max(0, x)$$\n",
    "$$f'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "### Softmax\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$$\n",
    "\n",
    "Converts raw scores to probability distribution over classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    ReLU activation function: f(x) = max(0, x)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Z : numpy array\n",
    "        Input to the activation function\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    A : numpy array\n",
    "        Output after applying ReLU\n",
    "    \"\"\"\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU function\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Z : numpy array\n",
    "        Input to the activation function\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dZ : numpy array\n",
    "        Gradient of ReLU\n",
    "    \"\"\"\n",
    "    return (Z > 0).astype(float)\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Softmax activation function\n",
    "    Converts scores to probability distribution\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Z : numpy array of shape (batch_size, num_classes)\n",
    "        Raw scores from the output layer\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    A : numpy array of shape (batch_size, num_classes)\n",
    "        Probabilities for each class (sum to 1 across classes)\n",
    "    \"\"\"\n",
    "    # Subtract max for numerical stability (prevents overflow)\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "# Test activation functions\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING ACTIVATION FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test ReLU\n",
    "test_input = np.array([[-2, -1, 0, 1, 2]])\n",
    "print(\"\\nReLU Test:\")\n",
    "print(f\"  Input:  {test_input[0]}\")\n",
    "print(f\"  Output: {relu(test_input)[0]}\")\n",
    "print(f\"  Derivative: {relu_derivative(test_input)[0]}\")\n",
    "\n",
    "# Test Softmax\n",
    "test_scores = np.array([[2.0, 1.0, 0.1]])\n",
    "softmax_output = softmax(test_scores)\n",
    "print(\"\\nSoftmax Test:\")\n",
    "print(f\"  Input scores: {test_scores[0]}\")\n",
    "print(f\"  Output probabilities: {softmax_output[0]}\")\n",
    "print(f\"  Sum of probabilities: {np.sum(softmax_output):.6f} (should be 1.0)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f71ee09",
   "metadata": {},
   "source": [
    "## 6. Implement Forward Propagation\n",
    "\n",
    "Forward propagation computes the output of the neural network given an input.\n",
    "\n",
    "### Mathematical Steps:\n",
    "\n",
    "**Layer 1 (Input â†’ Hidden):**\n",
    "$$Z_1 = XW_1 + b_1$$\n",
    "$$A_1 = \\text{ReLU}(Z_1) = \\max(0, Z_1)$$\n",
    "\n",
    "**Layer 2 (Hidden â†’ Output):**\n",
    "$$Z_2 = A_1W_2 + b_2$$\n",
    "$$A_2 = \\text{Softmax}(Z_2)$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the input (batch of images)\n",
    "- $W_1, b_1$ are weights and biases for hidden layer\n",
    "- $W_2, b_2$ are weights and biases for output layer\n",
    "- $A_1$ is the activation of hidden layer\n",
    "- $A_2$ is the final output (class probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Perform forward propagation through the neural network\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy array of shape (batch_size, 784)\n",
    "        Input data (flattened images)\n",
    "    parameters : dict\n",
    "        Dictionary containing W1, b1, W2, b2\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    A2 : numpy array of shape (batch_size, 10)\n",
    "        Output layer activations (class probabilities)\n",
    "    cache : dict\n",
    "        Intermediate values needed for backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract parameters\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Layer 1: Input â†’ Hidden\n",
    "    # Z1 = X @ W1 + b1\n",
    "    Z1 = np.dot(X, W1) + b1        # Shape: (batch_size, 128)\n",
    "    A1 = relu(Z1)                   # Shape: (batch_size, 128)\n",
    "    \n",
    "    # Layer 2: Hidden â†’ Output\n",
    "    # Z2 = A1 @ W2 + b2\n",
    "    Z2 = np.dot(A1, W2) + b2       # Shape: (batch_size, 10)\n",
    "    A2 = softmax(Z2)                # Shape: (batch_size, 10)\n",
    "    \n",
    "    # Store values for backpropagation\n",
    "    cache = {\n",
    "        'X': X,\n",
    "        'Z1': Z1,\n",
    "        'A1': A1,\n",
    "        'Z2': Z2,\n",
    "        'A2': A2\n",
    "    }\n",
    "    \n",
    "    return A2, cache\n",
    "\n",
    "# Test forward propagation with a small batch\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING FORWARD PROPAGATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with 5 random samples\n",
    "test_batch = X_train[:5]\n",
    "predictions, cache = forward_propagation(test_batch, parameters)\n",
    "\n",
    "print(f\"\\nInput shape: {test_batch.shape}\")\n",
    "print(f\"Output shape: {predictions.shape}\")\n",
    "print(f\"\\nPredictions (probabilities for each class):\")\n",
    "for i in range(5):\n",
    "    predicted_digit = np.argmax(predictions[i])\n",
    "    confidence = predictions[i][predicted_digit]\n",
    "    actual_digit = np.argmax(Y_train[i])\n",
    "    print(f\"  Sample {i+1}: Predicted={predicted_digit} (confidence={confidence:.4f}), Actual={actual_digit}\")\n",
    "    \n",
    "print(f\"\\nCache contains: {list(cache.keys())}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38679d",
   "metadata": {},
   "source": [
    "## 7. Implement Loss Function\n",
    "\n",
    "We use **Cross-Entropy Loss** for multi-class classification:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{j=1}^{C} y_{ij} \\log(\\hat{y}_{ij})$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the batch size\n",
    "- $C$ is the number of classes (10)\n",
    "- $y_{ij}$ is the true label (one-hot encoded)\n",
    "- $\\hat{y}_{ij}$ is the predicted probability\n",
    "- The loss measures how different predictions are from true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e21ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(A2, Y):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A2 : numpy array of shape (batch_size, 10)\n",
    "        Predicted probabilities from output layer\n",
    "    Y : numpy array of shape (batch_size, 10)\n",
    "        True labels (one-hot encoded)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    loss : float\n",
    "        Cross-entropy loss value\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]  # Number of samples\n",
    "    \n",
    "    # Add small epsilon to prevent log(0)\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    # Cross-entropy loss: -1/m * sum(y * log(y_hat))\n",
    "    loss = -np.sum(Y * np.log(A2 + epsilon)) / m\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def compute_accuracy(A2, Y):\n",
    "    \"\"\"\n",
    "    Compute classification accuracy\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A2 : numpy array of shape (batch_size, 10)\n",
    "        Predicted probabilities\n",
    "    Y : numpy array of shape (batch_size, 10)\n",
    "        True labels (one-hot encoded)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    accuracy : float\n",
    "        Percentage of correct predictions\n",
    "    \"\"\"\n",
    "    # Get predicted class (highest probability)\n",
    "    predictions = np.argmax(A2, axis=1)\n",
    "    \n",
    "    # Get true class\n",
    "    true_labels = np.argmax(Y, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predictions == true_labels) * 100\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Test loss function\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING LOSS FUNCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_predictions, _ = forward_propagation(X_train[:100], parameters)\n",
    "test_labels = Y_train[:100]\n",
    "\n",
    "loss = compute_loss(test_predictions, test_labels)\n",
    "accuracy = compute_accuracy(test_predictions, test_labels)\n",
    "\n",
    "print(f\"\\nTest on 100 samples (untrained network):\")\n",
    "print(f\"  Loss: {loss:.4f}\")\n",
    "print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"  Random guessing would give ~10% accuracy\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77532d33",
   "metadata": {},
   "source": [
    "## 8. Implement Backpropagation\n",
    "\n",
    "Backpropagation computes gradients of the loss with respect to all parameters using the **chain rule**.\n",
    "\n",
    "### Mathematical Derivation:\n",
    "\n",
    "**Output Layer Gradients:**\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial Z_2} = A_2 - Y$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W_2} = \\frac{1}{m} A_1^T (A_2 - Y)$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b_2} = \\frac{1}{m} \\sum (A_2 - Y)$$\n",
    "\n",
    "**Hidden Layer Gradients:**\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial Z_1} = \\frac{\\partial \\mathcal{L}}{\\partial Z_2} \\cdot W_2^T \\odot \\text{ReLU}'(Z_1)$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W_1} = \\frac{1}{m} X^T \\frac{\\partial \\mathcal{L}}{\\partial Z_1}$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b_1} = \\frac{1}{m} \\sum \\frac{\\partial \\mathcal{L}}{\\partial Z_1}$$\n",
    "\n",
    "Where $\\odot$ denotes element-wise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f894f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(cache, Y, parameters):\n",
    "    \"\"\"\n",
    "    Perform backward propagation to compute gradients\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cache : dict\n",
    "        Contains intermediate values from forward propagation\n",
    "    Y : numpy array of shape (batch_size, 10)\n",
    "        True labels (one-hot encoded)\n",
    "    parameters : dict\n",
    "        Current weights and biases\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    gradients : dict\n",
    "        Dictionary containing gradients for all parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[0]  # Batch size\n",
    "    \n",
    "    # Extract values from cache\n",
    "    X = cache['X']\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    Z1 = cache['Z1']\n",
    "    \n",
    "    # Extract parameters\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    # ==========================================\n",
    "    # BACKWARD PASS - Layer 2 (Output Layer)\n",
    "    # ==========================================\n",
    "    \n",
    "    # Gradient of loss with respect to Z2\n",
    "    # For softmax + cross-entropy: dZ2 = A2 - Y\n",
    "    dZ2 = A2 - Y  # Shape: (batch_size, 10)\n",
    "    \n",
    "    # Gradient of loss with respect to W2\n",
    "    # dW2 = (1/m) * A1^T @ dZ2\n",
    "    dW2 = (1/m) * np.dot(A1.T, dZ2)  # Shape: (128, 10)\n",
    "    \n",
    "    # Gradient of loss with respect to b2\n",
    "    # db2 = (1/m) * sum(dZ2) across samples\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)  # Shape: (1, 10)\n",
    "    \n",
    "    # ==========================================\n",
    "    # BACKWARD PASS - Layer 1 (Hidden Layer)\n",
    "    # ==========================================\n",
    "    \n",
    "    # Gradient flowing back from layer 2\n",
    "    # dA1 = dZ2 @ W2^T\n",
    "    dA1 = np.dot(dZ2, W2.T)  # Shape: (batch_size, 128)\n",
    "    \n",
    "    # Apply ReLU derivative\n",
    "    # dZ1 = dA1 * ReLU'(Z1)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)  # Shape: (batch_size, 128)\n",
    "    \n",
    "    # Gradient of loss with respect to W1\n",
    "    # dW1 = (1/m) * X^T @ dZ1\n",
    "    dW1 = (1/m) * np.dot(X.T, dZ1)  # Shape: (784, 128)\n",
    "    \n",
    "    # Gradient of loss with respect to b1\n",
    "    # db1 = (1/m) * sum(dZ1) across samples\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)  # Shape: (1, 128)\n",
    "    \n",
    "    # Store all gradients\n",
    "    gradients = {\n",
    "        'dW1': dW1,\n",
    "        'db1': db1,\n",
    "        'dW2': dW2,\n",
    "        'db2': db2\n",
    "    }\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Test backpropagation\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING BACKPROPAGATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Forward pass\n",
    "test_batch = X_train[:32]\n",
    "test_labels = Y_train[:32]\n",
    "predictions, cache = forward_propagation(test_batch, parameters)\n",
    "\n",
    "# Backward pass\n",
    "gradients = backward_propagation(cache, test_labels, parameters)\n",
    "\n",
    "print(f\"\\nGradient Shapes:\")\n",
    "print(f\"  dW1: {gradients['dW1'].shape} (should match W1: {parameters['W1'].shape})\")\n",
    "print(f\"  db1: {gradients['db1'].shape} (should match b1: {parameters['b1'].shape})\")\n",
    "print(f\"  dW2: {gradients['dW2'].shape} (should match W2: {parameters['W2'].shape})\")\n",
    "print(f\"  db2: {gradients['db2'].shape} (should match b2: {parameters['b2'].shape})\")\n",
    "\n",
    "print(f\"\\nSample Gradient Statistics:\")\n",
    "print(f\"  dW1 - mean: {np.mean(gradients['dW1']):.6f}, std: {np.std(gradients['dW1']):.6f}\")\n",
    "print(f\"  dW2 - mean: {np.mean(gradients['dW2']):.6f}, std: {np.std(gradients['dW2']):.6f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc323fe6",
   "metadata": {},
   "source": [
    "## 9. Implement Gradient Descent\n",
    "\n",
    "Gradient descent updates parameters in the direction that reduces the loss:\n",
    "\n",
    "$$W := W - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial W}$$\n",
    "$$b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$$\n",
    "\n",
    "Where $\\alpha$ is the **learning rate** that controls the step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd4a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    parameters : dict\n",
    "        Current weights and biases\n",
    "    gradients : dict\n",
    "        Gradients computed from backpropagation\n",
    "    learning_rate : float\n",
    "        Step size for parameter updates\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    parameters : dict\n",
    "        Updated weights and biases\n",
    "    \"\"\"\n",
    "    \n",
    "    # Update weights and biases for layer 1\n",
    "    parameters['W1'] = parameters['W1'] - learning_rate * gradients['dW1']\n",
    "    parameters['b1'] = parameters['b1'] - learning_rate * gradients['db1']\n",
    "    \n",
    "    # Update weights and biases for layer 2\n",
    "    parameters['W2'] = parameters['W2'] - learning_rate * gradients['dW2']\n",
    "    parameters['b2'] = parameters['b2'] - learning_rate * gradients['db2']\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Test parameter update\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING PARAMETER UPDATE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store original parameter values\n",
    "original_W1 = parameters['W1'].copy()\n",
    "\n",
    "# Perform one update\n",
    "learning_rate = 0.01\n",
    "parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "\n",
    "# Check if parameters changed\n",
    "param_change = np.mean(np.abs(parameters['W1'] - original_W1))\n",
    "print(f\"\\nLearning rate: {learning_rate}\")\n",
    "print(f\"Average change in W1: {param_change:.8f}\")\n",
    "print(\"Parameters updated successfully!\")\n",
    "\n",
    "# Re-initialize for training\n",
    "parameters = initialize_parameters(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "print(\"\\nParameters re-initialized for training\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727e329",
   "metadata": {},
   "source": [
    "## 10. Train the Neural Network\n",
    "\n",
    "Now we combine all components to train the neural network using **mini-batch gradient descent**.\n",
    "\n",
    "### Training Process:\n",
    "1. Split training data into mini-batches\n",
    "2. For each epoch:\n",
    "   - Shuffle the data\n",
    "   - For each mini-batch:\n",
    "     - Forward propagation\n",
    "     - Compute loss\n",
    "     - Backward propagation\n",
    "     - Update parameters\n",
    "   - Track loss and accuracy\n",
    "\n",
    "### Hyperparameters:\n",
    "- **Epochs**: 10 (number of complete passes through training data)\n",
    "- **Batch size**: 128 (number of samples per mini-batch)\n",
    "- **Learning rate**: 0.1 (step size for gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2443cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X_train, Y_train, X_val, Y_val, epochs=10, batch_size=128, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Train the neural network using mini-batch gradient descent\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : numpy array\n",
    "        Training data\n",
    "    Y_train : numpy array\n",
    "        Training labels (one-hot encoded)\n",
    "    X_val : numpy array\n",
    "        Validation data\n",
    "    Y_val : numpy array\n",
    "        Validation labels (one-hot encoded)\n",
    "    epochs : int\n",
    "        Number of complete passes through training data\n",
    "    batch_size : int\n",
    "        Number of samples per mini-batch\n",
    "    learning_rate : float\n",
    "        Step size for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    parameters : dict\n",
    "        Trained weights and biases\n",
    "    history : dict\n",
    "        Training history (loss and accuracy over epochs)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    num_samples = X_train.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"TRAINING NEURAL NETWORK\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training samples: {num_samples}\")\n",
    "    print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Batches per epoch: {num_batches}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        X_shuffled = X_train[indices]\n",
    "        Y_shuffled = Y_train[indices]\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_correct = 0\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for batch in range(num_batches):\n",
    "            # Get mini-batch\n",
    "            start_idx = batch * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            X_batch = X_shuffled[start_idx:end_idx]\n",
    "            Y_batch = Y_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            # Forward propagation\n",
    "            A2, cache = forward_propagation(X_batch, parameters)\n",
    "            \n",
    "            # Compute loss\n",
    "            batch_loss = compute_loss(A2, Y_batch)\n",
    "            epoch_loss += batch_loss\n",
    "            \n",
    "            # Count correct predictions\n",
    "            predictions = np.argmax(A2, axis=1)\n",
    "            true_labels = np.argmax(Y_batch, axis=1)\n",
    "            epoch_correct += np.sum(predictions == true_labels)\n",
    "            \n",
    "            # Backward propagation\n",
    "            gradients = backward_propagation(cache, Y_batch, parameters)\n",
    "            \n",
    "            # Update parameters\n",
    "            parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        train_loss = epoch_loss / num_batches\n",
    "        train_acc = (epoch_correct / num_samples) * 100\n",
    "        \n",
    "        # Validation\n",
    "        val_predictions, _ = forward_propagation(X_val, parameters)\n",
    "        val_loss = compute_loss(val_predictions, Y_val)\n",
    "        val_acc = compute_accuracy(val_predictions, Y_val)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return parameters, history\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "split_idx = 50000\n",
    "X_train_split = X_train[:split_idx]\n",
    "Y_train_split = Y_train[:split_idx]\n",
    "X_val = X_train[split_idx:]\n",
    "Y_val = Y_train[split_idx:]\n",
    "\n",
    "print(f\"Training set: {X_train_split.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print()\n",
    "\n",
    "# Train the network\n",
    "trained_parameters, training_history = train_neural_network(\n",
    "    X_train_split, \n",
    "    Y_train_split, \n",
    "    X_val, \n",
    "    Y_val,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c4960",
   "metadata": {},
   "source": [
    "### Visualize Training Progress\n",
    "\n",
    "Let's plot the training and validation loss/accuracy over epochs to see how well the model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a94e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "epochs_range = range(1, len(training_history['train_loss']) + 1)\n",
    "ax1.plot(epochs_range, training_history['train_loss'], 'b-o', label='Training Loss', linewidth=2)\n",
    "ax1.plot(epochs_range, training_history['val_loss'], 'r-s', label='Validation Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(epochs_range, training_history['train_acc'], 'b-o', label='Training Accuracy', linewidth=2)\n",
    "ax2.plot(epochs_range, training_history['val_acc'], 'r-s', label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Training Accuracy: {training_history['train_acc'][-1]:.2f}%\")\n",
    "print(f\"Final Validation Accuracy: {training_history['val_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb248924",
   "metadata": {},
   "source": [
    "## 11. Evaluate Model Accuracy\n",
    "\n",
    "Let's evaluate the trained model on the test dataset to see how well it generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c33e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATING MODEL ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions, _ = forward_propagation(X_test, trained_parameters)\n",
    "test_loss = compute_loss(test_predictions, Y_test)\n",
    "test_acc = compute_accuracy(test_predictions, Y_test)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"  Correct predictions: {int(test_acc * len(X_test) / 100)} / {len(X_test)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show some correct and incorrect predictions\n",
    "predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "true_labels = np.argmax(Y_test, axis=1)\n",
    "correct_mask = (predicted_labels == true_labels)\n",
    "incorrect_mask = ~correct_mask\n",
    "\n",
    "# Visualize correct predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "fig.suptitle('Correct Predictions', fontsize=16, fontweight='bold')\n",
    "\n",
    "correct_indices = np.where(correct_mask)[0][:10]\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = correct_indices[i]\n",
    "    ax.imshow(test_images[idx], cmap='gray')\n",
    "    confidence = test_predictions[idx][predicted_labels[idx]]\n",
    "    ax.set_title(f'Predicted: {predicted_labels[idx]}\\nTrue: {true_labels[idx]}\\nConf: {confidence:.2f}', \n",
    "                 fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize incorrect predictions\n",
    "if np.sum(incorrect_mask) > 0:\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "    fig.suptitle('Incorrect Predictions (if any)', fontsize=16, fontweight='bold', color='red')\n",
    "    \n",
    "    incorrect_indices = np.where(incorrect_mask)[0][:10]\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(incorrect_indices):\n",
    "            idx = incorrect_indices[i]\n",
    "            ax.imshow(test_images[idx], cmap='gray')\n",
    "            confidence = test_predictions[idx][predicted_labels[idx]]\n",
    "            ax.set_title(f'Predicted: {predicted_labels[idx]}\\nTrue: {true_labels[idx]}\\nConf: {confidence:.2f}', \n",
    "                         fontsize=10, color='red')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0a146",
   "metadata": {},
   "source": [
    "## 12. Load and Preprocess Custom Image\n",
    "\n",
    "This section implements the image input system to accept handwritten digit images and prepare them for prediction.\n",
    "\n",
    "### Image Preprocessing Steps:\n",
    "1. Load the image file (PNG, JPG, JPEG supported)\n",
    "2. Convert to grayscale\n",
    "3. Resize to 28Ã—28 pixels\n",
    "4. Invert colors (if needed - MNIST digits are white on black background)\n",
    "5. Normalize pixel values to [0, 1]\n",
    "6. Flatten to 784-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe408b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, invert=True):\n",
    "    \"\"\"\n",
    "    Load and preprocess a custom handwritten digit image\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the image file\n",
    "    invert : bool\n",
    "        If True, invert colors (for images with black digits on white background)\n",
    "        MNIST expects white digits on black background\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    image_array : numpy array of shape (1, 784)\n",
    "        Preprocessed image ready for prediction\n",
    "    original_image : PIL Image\n",
    "        Original loaded image for visualization\n",
    "    processed_image : numpy array of shape (28, 28)\n",
    "        Processed image for visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load image\n",
    "    img = Image.open(image_path)\n",
    "    original_image = img.copy()\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    img = img.convert('L')\n",
    "    \n",
    "    # Resize to 28x28 pixels\n",
    "    img = img.resize((28, 28), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Invert colors if needed (MNIST has white digits on black background)\n",
    "    if invert:\n",
    "        img_array = 255 - img_array\n",
    "    \n",
    "    # Store for visualization\n",
    "    processed_image = img_array.copy()\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    img_array = img_array.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Flatten to 784-dimensional vector\n",
    "    img_array = img_array.reshape(1, 784)\n",
    "    \n",
    "    return img_array, original_image, processed_image\n",
    "\n",
    "# Save a sample image from test set to demonstrate\n",
    "sample_idx = 42\n",
    "sample_image = test_images[sample_idx]\n",
    "sample_pil = Image.fromarray(sample_image)\n",
    "sample_path = \"sample_digit.png\"\n",
    "sample_pil.save(sample_path)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CUSTOM IMAGE PREPROCESSING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSaved sample image: {sample_path}\")\n",
    "print(f\"Actual label: {test_labels[sample_idx]}\")\n",
    "print(\"\\nImage preprocessing function ready!\")\n",
    "print(\"\\nTo use with your own image:\")\n",
    "print(\"  1. Save your handwritten digit image\")\n",
    "print(\"  2. Call: load_and_preprocess_image('your_image.png')\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ab226",
   "metadata": {},
   "source": [
    "## 13. Make Prediction on Custom Image\n",
    "\n",
    "This section implements the prediction system that:\n",
    "1. Accepts a preprocessed image\n",
    "2. Runs forward propagation through the trained neural network\n",
    "3. Extracts the predicted digit\n",
    "4. Outputs the result in the specified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896af836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_digit(image_path, parameters, invert=True):\n",
    "    \"\"\"\n",
    "    Predict the digit in a custom image\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the image file\n",
    "    parameters : dict\n",
    "        Trained neural network parameters\n",
    "    invert : bool\n",
    "        Whether to invert colors\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    predicted_digit : int\n",
    "        The predicted digit (0-9)\n",
    "    confidence : float\n",
    "        Confidence score for the prediction\n",
    "    probabilities : numpy array\n",
    "        Probability distribution over all classes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    image_array, original_image, processed_image = load_and_preprocess_image(image_path, invert)\n",
    "    \n",
    "    # Forward propagation to get predictions\n",
    "    predictions, _ = forward_propagation(image_array, parameters)\n",
    "    \n",
    "    # Extract predicted digit (class with highest probability)\n",
    "    predicted_digit = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_digit]\n",
    "    probabilities = predictions[0]\n",
    "    \n",
    "    return predicted_digit, confidence, probabilities, original_image, processed_image\n",
    "\n",
    "# Test with a sample image\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING PREDICTION ON CUSTOM IMAGE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Predict on the sample image we saved\n",
    "predicted_digit, confidence, probabilities, orig_img, proc_img = predict_digit(\n",
    "    \"sample_digit.png\", \n",
    "    trained_parameters, \n",
    "    invert=False  # MNIST images are already white on black\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"The uploaded handwritten digit is recognized as: {predicted_digit}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nConfidence: {confidence:.4f} ({confidence*100:.2f}%)\")\n",
    "print(f\"\\nProbability distribution across all digits:\")\n",
    "for digit in range(10):\n",
    "    bar = 'â–ˆ' * int(probabilities[digit] * 50)\n",
    "    print(f\"  Digit {digit}: {probabilities[digit]:.4f} {bar}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2587fc8f",
   "metadata": {},
   "source": [
    "## 14. Display Prediction Result\n",
    "\n",
    "Visualize the uploaded image alongside the prediction result with confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f22380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_prediction(image_path, parameters, invert=True):\n",
    "    \"\"\"\n",
    "    Display the image and prediction result in a nice visualization\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the image file\n",
    "    parameters : dict\n",
    "        Trained neural network parameters\n",
    "    invert : bool\n",
    "        Whether to invert colors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get prediction\n",
    "    predicted_digit, confidence, probabilities, orig_img, proc_img = predict_digit(\n",
    "        image_path, parameters, invert\n",
    "    )\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Original image\n",
    "    ax1 = plt.subplot(1, 3, 1)\n",
    "    ax1.imshow(orig_img, cmap='gray')\n",
    "    ax1.set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Processed image (28x28)\n",
    "    ax2 = plt.subplot(1, 3, 2)\n",
    "    ax2.imshow(proc_img, cmap='gray')\n",
    "    ax2.set_title('Processed Image (28Ã—28)', fontsize=14, fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Probability bar chart\n",
    "    ax3 = plt.subplot(1, 3, 3)\n",
    "    digits = np.arange(10)\n",
    "    colors = ['green' if i == predicted_digit else 'blue' for i in digits]\n",
    "    bars = ax3.bar(digits, probabilities, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    bars[predicted_digit].set_color('red')\n",
    "    bars[predicted_digit].set_alpha(1.0)\n",
    "    ax3.set_xlabel('Digit', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Probability', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Class Probabilities', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xticks(digits)\n",
    "    ax3.set_ylim([0, 1])\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the required output format\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"The uploaded handwritten digit is recognized as: {predicted_digit}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Confidence: {confidence*100:.2f}%\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Display the prediction for our sample image\n",
    "display_prediction(\"sample_digit.png\", trained_parameters, invert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb6ad82",
   "metadata": {},
   "source": [
    "### Test with Multiple Images\n",
    "\n",
    "Let's test the system with multiple test images to demonstrate its capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08239f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test multiple sample images\n",
    "sample_indices = [7, 42, 123, 456, 789, 1111]\n",
    "sample_images_paths = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CREATING SAMPLE TEST IMAGES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    img = test_images[idx]\n",
    "    label = test_labels[idx]\n",
    "    path = f\"test_digit_{label}_sample_{idx}.png\"\n",
    "    Image.fromarray(img).save(path)\n",
    "    sample_images_paths.append(path)\n",
    "    print(f\"Created: {path} (actual label: {label})\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nTESTING PREDICTIONS ON MULTIPLE IMAGES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test each image\n",
    "for img_path in sample_images_paths:\n",
    "    predicted_digit, confidence, _, _, _ = predict_digit(img_path, trained_parameters, invert=False)\n",
    "    print(f\"\\nFile: {img_path}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"The uploaded handwritten digit is recognized as: {predicted_digit}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Confidence: {confidence*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL PREDICTIONS COMPLETED\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8720ec42",
   "metadata": {},
   "source": [
    "### Save the Trained Model\n",
    "\n",
    "Save the trained neural network parameters for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3be457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained parameters\n",
    "model_path = \"trained_digit_recognizer.pkl\"\n",
    "\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(trained_parameters, f)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL SAVED\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModel saved to: {model_path}\")\n",
    "print(\"\\nTo load the model later:\")\n",
    "print(\"```python\")\n",
    "print(\"with open('trained_digit_recognizer.pkl', 'rb') as f:\")\n",
    "print(\"    loaded_parameters = pickle.load(f)\")\n",
    "print(\"```\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Demonstrate loading\n",
    "with open(model_path, 'rb') as f:\n",
    "    loaded_parameters = pickle.load(f)\n",
    "\n",
    "print(\"\\nModel loaded successfully!\")\n",
    "print(\"Verifying loaded model with a test prediction...\")\n",
    "\n",
    "# Test loaded model\n",
    "test_pred, test_conf, _, _, _ = predict_digit(sample_images_paths[0], loaded_parameters, invert=False)\n",
    "print(f\"Test prediction: {test_pred} (confidence: {test_conf*100:.2f}%)\")\n",
    "print(\"Loaded model works correctly!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de2784",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ How to Use This System with Your Own Images\n",
    "\n",
    "### Step 1: Prepare Your Image\n",
    "- Draw or scan a handwritten digit (0-9)\n",
    "- Save it as PNG, JPG, or JPEG format\n",
    "- The digit should be clearly visible\n",
    "\n",
    "### Step 2: Load and Predict\n",
    "```python\n",
    "# For images with BLACK digit on WHITE background (most common):\n",
    "display_prediction(\"your_image.png\", trained_parameters, invert=True)\n",
    "\n",
    "# For images with WHITE digit on BLACK background (like MNIST):\n",
    "display_prediction(\"your_image.png\", trained_parameters, invert=False)\n",
    "```\n",
    "\n",
    "### Step 3: View Results\n",
    "The system will display:\n",
    "- Original image\n",
    "- Preprocessed 28Ã—28 image\n",
    "- Probability distribution for all digits\n",
    "- **Output message**: \"The uploaded handwritten digit is recognized as: X\"\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” System Components Summary\n",
    "\n",
    "### âœ… What We Implemented (From Scratch):\n",
    "\n",
    "1. **Neural Network Architecture**\n",
    "   - Input Layer: 784 neurons\n",
    "   - Hidden Layer: 128 neurons with ReLU activation\n",
    "   - Output Layer: 10 neurons with Softmax activation\n",
    "\n",
    "2. **Forward Propagation**\n",
    "   - Linear transformations (matrix multiplication)\n",
    "   - Activation functions (ReLU, Softmax)\n",
    "\n",
    "3. **Loss Function**\n",
    "   - Cross-entropy loss for multi-class classification\n",
    "\n",
    "4. **Backpropagation**\n",
    "   - Gradient computation using chain rule\n",
    "   - Layer-by-layer gradient flow\n",
    "\n",
    "5. **Optimization**\n",
    "   - Mini-batch gradient descent\n",
    "   - Parameter updates with learning rate\n",
    "\n",
    "6. **Image Processing**\n",
    "   - Load, resize, normalize images\n",
    "   - Convert to neural network input format\n",
    "\n",
    "7. **Prediction System**\n",
    "   - Forward pass on custom images\n",
    "   - Output in specified format\n",
    "\n",
    "### ðŸ“Š Performance Achieved:\n",
    "- Test Accuracy: ~95-97% (typical for this architecture)\n",
    "- Training Time: ~10 epochs\n",
    "- No TensorFlow, PyTorch, or Keras used!\n",
    "\n",
    "### ðŸš€ Libraries Used (Only Basic Tools):\n",
    "- **NumPy**: Matrix operations and numerical computations\n",
    "- **Matplotlib**: Visualization\n",
    "- **PIL (Pillow)**: Image loading and preprocessing\n",
    "- **pickle**: Model saving/loading\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8349882",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Quick Start Example\n",
    "\n",
    "Use this cell to quickly test with your own image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CUSTOMIZE THIS SECTION FOR YOUR IMAGE\n",
    "# ========================================\n",
    "\n",
    "# Replace this with the path to your image\n",
    "YOUR_IMAGE_PATH = \"sample_digit.png\"\n",
    "\n",
    "# Set to True if your image has BLACK digit on WHITE background\n",
    "# Set to False if your image has WHITE digit on BLACK background\n",
    "INVERT_COLORS = False\n",
    "\n",
    "# ========================================\n",
    "# RUN PREDICTION\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"ðŸ” ANALYZING YOUR HANDWRITTEN DIGIT...\" + \"\\n\")\n",
    "\n",
    "try:\n",
    "    # Make prediction\n",
    "    predicted_digit, confidence, probabilities, orig_img, proc_img = predict_digit(\n",
    "        YOUR_IMAGE_PATH, \n",
    "        trained_parameters, \n",
    "        invert=INVERT_COLORS\n",
    "    )\n",
    "    \n",
    "    # Display visualization\n",
    "    display_prediction(YOUR_IMAGE_PATH, trained_parameters, invert=INVERT_COLORS)\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(f\"\\nâœ… PREDICTION SUCCESSFUL!\")\n",
    "    print(f\"\\nTop 3 predictions:\")\n",
    "    top_3_indices = np.argsort(probabilities)[-3:][::-1]\n",
    "    for i, idx in enumerate(top_3_indices, 1):\n",
    "        print(f\"  {i}. Digit {idx}: {probabilities[idx]*100:.2f}%\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ Error: Image file '{YOUR_IMAGE_PATH}' not found!\")\n",
    "    print(\"\\nPlease:\")\n",
    "    print(\"  1. Save your handwritten digit image in the same directory\")\n",
    "    print(\"  2. Update YOUR_IMAGE_PATH variable above\")\n",
    "    print(\"  3. Run this cell again\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d57fc5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ Mathematical Concepts Implemented\n",
    "\n",
    "### 1. Forward Propagation\n",
    "$$Z_1 = XW_1 + b_1$$\n",
    "$$A_1 = \\text{ReLU}(Z_1) = \\max(0, Z_1)$$\n",
    "$$Z_2 = A_1W_2 + b_2$$\n",
    "$$A_2 = \\text{Softmax}(Z_2) = \\frac{e^{Z_2}}{\\sum e^{Z_2}}$$\n",
    "\n",
    "### 2. Cross-Entropy Loss\n",
    "$$\\mathcal{L} = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{j=1}^{10} y_{ij} \\log(\\hat{y}_{ij})$$\n",
    "\n",
    "### 3. Backpropagation Gradients\n",
    "**Output Layer:**\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial Z_2} = A_2 - Y$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W_2} = \\frac{1}{m} A_1^T (A_2 - Y)$$\n",
    "\n",
    "**Hidden Layer:**\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial Z_1} = \\frac{\\partial \\mathcal{L}}{\\partial Z_2} W_2^T \\odot \\text{ReLU}'(Z_1)$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W_1} = \\frac{1}{m} X^T \\frac{\\partial \\mathcal{L}}{\\partial Z_1}$$\n",
    "\n",
    "### 4. Gradient Descent Update\n",
    "$$W := W - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial W}$$\n",
    "$$b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ Congratulations!\n",
    "\n",
    "You have successfully implemented a complete neural network from scratch without using any deep learning libraries! This notebook demonstrates:\n",
    "\n",
    "âœ… **Neural Network Fundamentals**: Forward/backward propagation, activation functions  \n",
    "âœ… **Optimization**: Gradient descent, mini-batch training  \n",
    "âœ… **Image Processing**: Loading, preprocessing, normalization  \n",
    "âœ… **Prediction System**: Custom image input with specified output format  \n",
    "âœ… **High Accuracy**: ~95-97% on MNIST test set  \n",
    "\n",
    "**Next Steps:**\n",
    "- Try with your own handwritten digits\n",
    "- Experiment with different architectures (more layers, different sizes)\n",
    "- Implement additional features (momentum, learning rate decay)\n",
    "- Try on other datasets\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b9797",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸŽ¯ Test Your Own Image Now!\n",
    "\n",
    "Now that your model is trained, use this section to test it with your own handwritten digit images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b0558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ“¤ UPLOAD AND TEST YOUR IMAGE HERE\n",
    "# ============================================================\n",
    "# \n",
    "# STEP 1: Save your handwritten digit image in this folder\n",
    "# STEP 2: Change the filename below to match your image\n",
    "# STEP 3: Run this cell to get the prediction!\n",
    "#\n",
    "# ============================================================\n",
    "\n",
    "# ðŸ‘‰ CHANGE THIS TO YOUR IMAGE FILENAME:\n",
    "my_image = \"my_digit.png\"  # e.g., \"my_digit.png\", \"test.jpg\", \"handwritten_5.png\"\n",
    "\n",
    "# ðŸ‘‰ SET COLOR INVERSION:\n",
    "# - True: if you have BLACK digit on WHITE background (most common)\n",
    "# - False: if you have WHITE digit on BLACK background (like MNIST)\n",
    "invert = True\n",
    "\n",
    "# ============================================================\n",
    "# Prediction Code (Don't modify below unless you know what you're doing)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if not os.path.exists(my_image):\n",
    "    print(\"=\" * 70)\n",
    "    print(\"âŒ IMAGE NOT FOUND!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nLooking for: {my_image}\")\n",
    "    print(f\"In directory: {os.getcwd()}\")\n",
    "    print(\"\\nðŸ“ Instructions:\")\n",
    "    print(\"  1. Place your handwritten digit image in this folder\")\n",
    "    print(\"  2. Update 'my_image' variable above with your filename\")\n",
    "    print(\"  3. Run this cell again\")\n",
    "    print(\"\\nðŸ’¡ Tip: You can drag & drop your image into the file browser\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ðŸ” ANALYZING YOUR HANDWRITTEN DIGIT...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Make prediction\n",
    "    predicted_digit, confidence, probabilities, orig_img, proc_img = predict_digit(\n",
    "        my_image, \n",
    "        trained_parameters, \n",
    "        invert=invert\n",
    "    )\n",
    "    \n",
    "    # Display result\n",
    "    print(\"\\n\" + \"ðŸŽ‰ \" * 25)\n",
    "    print(f\"\\nThe uploaded handwritten digit is recognized as: {predicted_digit}\")\n",
    "    print(\"\\n\" + \"ðŸŽ‰ \" * 25)\n",
    "    print(f\"\\nConfidence Level: {confidence*100:.2f}%\")\n",
    "    \n",
    "    # Show top 3 predictions\n",
    "    print(\"\\nðŸ“Š Top 3 Most Likely Digits:\")\n",
    "    top_3_indices = np.argsort(probabilities)[-3:][::-1]\n",
    "    for rank, idx in enumerate(top_3_indices, 1):\n",
    "        bar = \"â–ˆ\" * int(probabilities[idx] * 40)\n",
    "        print(f\"  {rank}. Digit {idx}: {probabilities[idx]*100:5.2f}% {bar}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(orig_img, cmap='gray')\n",
    "    axes[0].set_title('ðŸ“· Your Original Image', fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Processed image\n",
    "    axes[1].imshow(proc_img, cmap='gray')\n",
    "    axes[1].set_title('ðŸ”§ Processed (28Ã—28)', fontsize=12, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Probability distribution\n",
    "    digits = np.arange(10)\n",
    "    colors = ['red' if i == predicted_digit else 'lightblue' for i in digits]\n",
    "    axes[2].bar(digits, probabilities, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "    axes[2].set_xlabel('Digit', fontsize=11, fontweight='bold')\n",
    "    axes[2].set_ylabel('Probability', fontsize=11, fontweight='bold')\n",
    "    axes[2].set_title(f'ðŸ“Š Prediction: {predicted_digit}', fontsize=12, fontweight='bold', color='red')\n",
    "    axes[2].set_xticks(digits)\n",
    "    axes[2].set_ylim([0, 1])\n",
    "    axes[2].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ… Prediction complete! Upload another image and run this cell again to test more.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd32e53",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Quick Alternative: One-Line Prediction\n",
    "\n",
    "If you want to quickly test different images, use this simple command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9287557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick one-line prediction: Just change the filename and run!\n",
    "# For BLACK digit on WHITE background:\n",
    "display_prediction(\"my_digit.png\", trained_parameters, invert=True)\n",
    "\n",
    "# For WHITE digit on BLACK background (like MNIST):\n",
    "# display_prediction(\"my_digit.png\", trained_parameters, invert=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}